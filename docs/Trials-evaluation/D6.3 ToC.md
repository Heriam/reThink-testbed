## D6.3 Table of Contents

### Introduction

### Trial Evaluations

#### Technical Evaluations

*tests for each rethink sub-system*

*template with common metrics*

##### Vertx Message Node (ALB)
###### Description of Component
Short description (1 paragraph, approx. 5-8 lines) describing the component under test
###### Metrics
Description of metrics used to test the components.  Should include:
  * What is tested (i.e. metric description)
  * For each metric:
    * reasoning why this metric matters for the component under test
    * expected performance (based on use cases under consideration, e.g. number of parallen interacting end-nodes, requests per second, delay of a querry-response, size of data exchanged etc; target values based on real-world deployments).

###### Tests
Description of test set up, may be a single set up or multiple set ups depending on the component under test and the metric being tested.
If you have a single test set-up, describe the test set up first and then include one sub-section per metric being tested.  If you need several different test set ups, use a "strucutre by metric", i.e., have one subsection per metric which in turn includes the description of the test set up for that metric and the results.




##### Matrix Message Node (Steffen, DT)
###### Description of Component
Short description (1 paragraph, approx. 5-8 lines) describing the component under test
###### Metrics
Description of metrics used to test the components.  Should include:
  * What is tested (i.e. metric description)
  * For each metric:
    * reasoning why this metric matters for the component under test
    * expected performance (based on use cases under consideration, e.g. number of parallen interacting end-nodes, requests per second, delay of a querry-response, size of data exchanged etc; target values based on real-world deployments).

###### Tests
Description of test set up, may be a single set up or multiple set ups depending on the component under test and the metric being tested.
If you have a single test set-up, describe the test set up first and then include one sub-section per metric being tested.  If you need several different test set ups, use a "strucutre by metric", i.e., have one subsection per metric which in turn includes the description of the test set up for that metric and the results.




##### NodeJS Message Node (Arnaut, APIZEE)
###### Description of Component
Short description (1 paragraph, approx. 5-8 lines) describing the component under test
###### Metrics
Description of metrics used to test the components.  Should include:
  * What is tested (i.e. metric description)
  * For each metric:
    * reasoning why this metric matters for the component under test
    * expected performance (based on use cases under consideration, e.g. number of parallen interacting end-nodes, requests per second, delay of a querry-response, size of data exchanged etc; target values based on real-world deployments).

###### Tests
Description of test set up, may be a single set up or multiple set ups depending on the component under test and the metric being tested.
If you have a single test set-up, describe the test set up first and then include one sub-section per metric being tested.  If you need several different test set ups, use a "strucutre by metric", i.e., have one subsection per metric which in turn includes the description of the test set up for that metric and the results.





##### Runtime (ALB)
###### Description of Component
Short description (1 paragraph, approx. 5-8 lines) describing the component under test
###### Metrics
Description of metrics used to test the components.  Should include:
  * What is tested (i.e. metric description)
  * For each metric:
    * reasoning why this metric matters for the component under test
    * expected performance (based on use cases under consideration, e.g. number of parallen interacting end-nodes, requests per second, delay of a querry-response, size of data exchanged etc; target values based on real-world deployments).

###### Tests
Description of test set up, may be a single set up or multiple set ups depending on the component under test and the metric being tested.
If you have a single test set-up, describe the test set up first and then include one sub-section per metric being tested.  If you need several different test set ups, use a "strucutre by metric", i.e., have one subsection per metric which in turn includes the description of the test set up for that metric and the results.






##### Domain Registry (INESC_ID)
###### Description of Component
Short description (1 paragraph, approx. 5-8 lines) describing the component under test
###### Metrics
Description of metrics used to test the components.  Should include:
  * What is tested (i.e. metric description)
  * For each metric:
    * reasoning why this metric matters for the component under test
    * expected performance (based on use cases under consideration, e.g. number of parallen interacting end-nodes, requests per second, delay of a querry-response, size of data exchanged etc; target values based on real-world deployments).

###### Tests
Description of test set up, may be a single set up or multiple set ups depending on the component under test and the metric being tested.
If you have a single test set-up, describe the test set up first and then include one sub-section per metric being tested.  If you need several different test set ups, use a "strucutre by metric", i.e., have one subsection per metric which in turn includes the description of the test set up for that metric and the results.





##### Catalogue (Marc, Fokus)
###### Description of Component
Short description (1 paragraph, approx. 5-8 lines) describing the component under test
###### Metrics
Description of metrics used to test the components.  Should include:
  * What is tested (i.e. metric description)
  * For each metric:
    * reasoning why this metric matters for the component under test
    * expected performance (based on use cases under consideration, e.g. number of parallen interacting end-nodes, requests per second, delay of a querry-response, size of data exchanged etc; target values based on real-world deployments).

###### Tests
Description of test set up, may be a single set up or multiple set ups depending on the component under test and the metric being tested.
If you have a single test set-up, describe the test set up first and then include one sub-section per metric being tested.  If you need several different test set ups, use a "strucutre by metric", i.e., have one subsection per metric which in turn includes the description of the test set up for that metric and the results.

##### IdPs (Orange)
###### Description of Component
Short description (1 paragraph, approx. 5-8 lines) describing the component under test
###### Metrics
Description of metrics used to test the components.  Should include:
  * What is tested (i.e. metric description)
  * For each metric:
    * reasoning why this metric matters for the component under test
    * expected performance (based on use cases under consideration, e.g. number of parallen interacting end-nodes, requests per second, delay of a querry-response, size of data exchanged etc; target values based on real-world deployments).

###### Tests
Description of test set up, may be a single set up or multiple set ups depending on the component under test and the metric being tested.
If you have a single test set-up, describe the test set up first and then include one sub-section per metric being tested.  If you need several different test set ups, use a "strucutre by metric", i.e., have one subsection per metric which in turn includes the description of the test set up for that metric and the results.




#### Development Evaluations (Ant√≥n, Quobis)

*Questionnaires*

### Assessment

#### Business Assessment

#### Architecture Assessment

##### Interoperability

*short description of test cases. Results summary and analysis*

##### Identity Management

*short description of test cases. Results summary and analysis*

##### Other functionalities
